\documentclass[pdftex,ptm,12pt,a4paper]{report}
\renewcommand{\baselinestretch}{1.5}
\setcounter{secnumdepth}{5}

% PDF search & cut'n'paste
\usepackage{cmap}
\usepackage[table,xcdraw]{xcolor}
\renewcommand{\baselinestretch}{1.5}
\usepackage{setspace}
\usepackage{indentfirst}

% Cyrillic support
\usepackage{mathtext}
\usepackage{amsmath}
\usepackage[T1,T2A]{fontenc}
\DeclareSymbolFont{T2Aletters}{T2A}{cmr}{m}{it}
\usepackage[utf8]{inputenc}
\usepackage{multicol}

\usepackage[english, russian]{babel}

\usepackage[bottom=30mm,top=20mm,right=20mm,left=30mm,headsep=0cm,nofoot]{geometry}

\usepackage{calc}
\setlength{\footskip}{\paperheight
  -(1in+\voffset+\topmargin+\headheight+\headsep+\textheight)
  -0.75in}

\usepackage{array}
\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}

\makeatletter
\renewcommand*{\ps@plain}{%
  \let\@mkboth\@gobbletwo
  \let\@oddhead\@empty
  \def\@oddfoot{%
    \reset@font
    \hfil
    \thepage
    % \hfil % removed for aligning to the right
  }%
  \let\@evenhead\@empty
  \let\@evenfoot\@oddfoot
}
\makeatother
\pagestyle{plain}

\usepackage[pdftex]{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage[russian,english]{babel}
    \addto{\captionsenglish}{\renewcommand{\bibname}{Литература}}
    \addto\captionsenglish{\renewcommand{\figurename}{Рис.}}
    \addto\captionsenglish{\renewcommand{\contentsname}{Содержание}}
    \addto\captionsenglish{\renewcommand{\proofname}{Доказательство}}
\usepackage{hyperref}
\usepackage{cleveref}
\usepackage{url}
\usepackage{abstract}
\usepackage{float}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{amsmath}
\renewcommand*{\proofname}{Доказательство}
\usepackage{indentfirst}
\usepackage{color}
\usepackage{natbib}
\usepackage{bbm, dsfont}


% Detect whether PDFLaTeX is in use
\usepackage{ifpdf}

% Fix links to floats
\usepackage[all]{hypcap}

\makeatletter
\renewcommand{\@chapapp}{Часть}
\makeatother

% Theorem Styles
\newtheorem{theorem}{Теорема}[chapter]
\newtheorem{lemma}[theorem]{Лемма}
\newtheorem{claim}[theorem]{Теорема}
% Definition Styles
\theoremstyle{definition}
\newtheorem{definition}{Определение}[chapter]
\newtheorem{example}{Пример}[chapter]
% Rule for Title Page
\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}

\begin{document}

\begin{titlepage}
\newpage

\begin{center}
МИНИСТЕРСТВО ОБРАЗОВАНИЯ И НАУКИ РОССИЙСКОЙ ФЕДЕРАЦИИ \\
\vspace{0.5cm}
ГОСУДАРСТВЕННОЕ ОБРАЗОВАТЕЛЬНОЕ УЧРЕЖДЕНИЕ \\*
ВЫСШЕГО ПРОФЕССИОНАЛЬНОГО ОБРАЗОВАНИЯ\\*
"МОСКОВСКИЙ ФИЗИКО-ТЕХНИЧЕСКИЙ ИНСТИТУТ \\*
(ГОСУДАРСТВЕННЫЙ УНИВЕРСИТЕТ)" \\*
\vspace{0.5cm}
ФАКУЛЬТЕТ ИННОВАЦИЙ И ВЫСОКИХ ТЕХНОЛОГИЙ \\*
КАФЕДРА АНАЛИЗА ДАННЫХ \\*
\hrulefill
\end{center}


\vspace{3em}

\begin{center}
\Large Выпускная квалификационная работа по направлению 01.03.02 <<Прикладная математика и информатика>> \linebreak НА ТЕМУ:
\end{center}

\vspace{2.5em}

\begin{center}
\textsc{\large{\textbf{Алгоритмы консенсуса на энергонезависимой памяти произвольного доступа}}}
\end{center}

\vspace{6.5em}

\begin{minipage}{.45\linewidth}
\begin{flushleft}
Студент \\ Научный руководитель к.ф-м.н \\ Зам. зав. кафедрой д.ф-м.н, проф.
\end{flushleft}
\end{minipage}
\hfill
\begin{minipage}{.45\linewidth}
\begin{flushright}
    Сурин М.С.\\ Бабенко А.В.\\Бунина Е.И.
\end{flushright}
\end{minipage}

\vspace{\fill}

\begin{center}
МОСКВА, 2020
\end{center}

\end{titlepage}

\tableofcontents
\sloppy

\setstretch{1.5}
\parindent=1.25cm

\chapter{Введение}
\section{NVDIMM}
Дизайн СУБД всегда был вынужден принимать во внимание принципиальную разницу между энергозависимыми устройствами хранения информации и энергонезависимыми.
Разница между типами устройств проявляется как в латентности доступа, так и в особенностях предоставляемого интерфейса.
Энергонезависимые устройства как правило предоставляют блочный интерфейс, то есть
позволяют оперировать только блоками из тысяч байт. В то время как современные DIMM дают возможность оперировать словами -- единицами байт.
Но не так давно (2014-15гг) появились на рынке технологии под общим названием “NVDIMM”, занимающие промежуточное положение: они сохраняют данные при потере питания,
предоставляя скорость доступа, сравнимую с DRAM и также давая возможность адресации отдельных байтов.

Большая часть продуктов на 2014г (например продукты Miсron Technology) представляют из себя модуль DRAM служащий кешeм для модуля энергонезависимой памяти (как правило NAND FLASH)
с автономным источником питания. Но тогда же были анонсированы, а позже и появились на рынке устройства без подобного разделения -- к примеру Intel Optane NVDIMM \cite{peng2019system}.

Типичные времена выполнения операция для промышленных устройств разных типов:
\begin{center}
\begin{tabular} {|l| c c c|}
\hline
    & SSD & NVDIMM & DIMM \\
    \hline
чтение & 200us & 120ns & 80ns \\
запись & 2ms & 750ns & 80ns \\
\hline
\end{tabular}
\end{center}

Такая небольшая разница между оперативной памятью и энергонезависимой подталкивает исследователей к пересмотру традиционных подходов к дизайну и архитектуре СУБД и перспективе даёт возможность добиться большей производительности.

\section{Обзор литературы}
Ещё до появления на рынке на рынке устройств, исследователи давно пытаются оптимизировать алгоритмы \cite{iwabuchi2014nvm} и структуры данных \cite{chen2015persistent} для персистентной памяти .
Многие из них пользуются либо программными платформами для эмуляции задержек NVDIMM \cite{sengupta2015framework} либо занимаются построением аппаратных моделей \cite{dong2012nvsim}.
Использование персистентной памяти ставит перед исследователями новые задачи, такие как менеджмент персистентной памяти \cite{schwalb2015nvm}. Также своя специфика есть у задачи обеспечения
целостности данных при потере питания, так как большинство современных процессоров кешируют доступы к памяти и практически не предоставляют примитивов для управления своим кешем.
Как следствие для работы с персистентной памятью появились отдельные библиотеки такие как уже ставшая стандартом для исследователей библиотека PMDK \cite{pmdk}
и ряд процессорных инструкций \cite{kolli2016delegated}.  Мы в своей работе в частности используем PMDK и сравниваем наши алгоритмы с построенной на её базе библиотекой pmemkv 
\cite{pmemkv} как и, большинство исследователей персистентных структур данных.
Задачей оптимизации архитектуры СУБД на персистентной памяти занимается удивительно малое количество исследователей; в основном можно выделить исследователей из Carnegie Mellon School of Computer Science
\cite{pavlo17}, \cite{arulraj2015let}, \cite{debrabant2014prolegomenon}, \cite{arulraj2017build}, \cite{writebehind}.  В \cite{writebehind} приведeна альтернатива write-ahead logging
для NVM, позволяющая повысить производительность СУБД и рассмотрена задача репликации с учётом NVM, но задача координации -- выбора master-реплики перекладывалась на внешнее отказоустойчивое хранилище.
Задачей исследования алгоритмов консенсуса для NVM по нашим сведениям никто не занимался.

\chapter{Цель работы}
\section{Постановка задачи}
Мы рассматриваем алгоритмы консенсуса с точки зрения задачи репликации состояния конечного автомата. Состояние будет представлять из себя ассоциативный массив, ключами и значениями
выступают произвольные последовательности байт. Зафиксируем интерфейс, достаточный для большинства прикладных задач:
\begin{itemize}
    \item get($key$) -- операция чтения значения по ключу
    \item set($key$, $value$) -- операция записи значения по ключу
    \item compare\_and\_set($key$, $value_1$, $value_2$) -- операция обновления значения по ключу $key$ на $value_2$ в случае если оно до операции равно $value_1$
\end{itemize}

От алгоритмов будем требовать линеаризуемости \cite{linearizability} истории операций в условиях наличия нарушений связности сети.

\section{Метрики}
При экспериментах мы изучаем следующие характеристики:
\begin{itemize}
    \item латентность записи (то есть операций set и compare\_and\_set)
    \item скорость восстановления отставших реплик
\end{itemize}

В большинстве прикладных реализаций алгоритмов репликации операции чтения обслуживается лидером без задействия реплик напрямую из своей оперативной памяти.
А значит, от алгоритмов оптимизированных для NVM нет поводов ожидать улучшений латентности чтения. Поэтому в нашей работе данную метрику не исследуем.

\section{Бейзлайн}
В качестве эталонной реализации мы используем реализацию алгоритма RAFT \cite{raftpaper} использующую NVDIMM как блочное устройство
для хранения write-ahead log и снимки состояния.

\label{baseline}
Напомним некоторые детали общей схемы работы данного алгоритма:
\begin{itemize}
    \item Алгоритм реплицирует состояние некоторого конечного автомата.
    \item Каждое изменение состояние получает уникальный номер от лидера -- далее timestamp.
    \item От лидера каждое изменение пересылается репликам.
    \item Реплики в некотором виде отказоустойчиво сохраняют присланные изменения и после отправляют лидеру timestamp последнего сохранённого изменения вместе с timestamp следующего ещё не
        полученного от лидера изменения. Изменения на репликах, отличающиеся от изменений полученных от лидера, при нахождении удаляются вместе
    с последующими сохранёнными соответствующей репликой.
    \item Лидер применяет изменение к своему состоянию после подтверждения того что оно сохранено более чем половиной участников.
    \item Лидер и реплики периодически обмениваются сообщениями и в случае неполучения такого сообщения в течение некоторого временного периода реплики инициируют перевыборы лидера.
    \item При выборах каждый участник пересылает предложение о голосовании за себя остальным, включая в него номер последнего сохранённого сообщения. В случае получения предложения с меньшим
    номером чем у получателя предложение отвергается. Новым лидером становится реплика, за которую проголосовало более половины участников. До того как приступить к обслуживанию
    запросов новый лидер рассылает и после применяет все свои изменения.
\end{itemize}

Кратко напомним общую схему работы write-ahead log:
\begin{itemize}
    \item Модификации накапливаются в памяти и периодическим процессом записываются на диск. Записи на диск подтверждаются одним из примитивов предоставляемых платформой (fsync/fdatasync в случае Linux).
    \item В случае пропажи питания и последующем перезапуске процесса, состояние процесса восстанавливается из записанных на диск модификаций. В качестве оптимизации
        с некоторой периодичностью на диск записываются снимки состояния в памяти и при восстановлении модификации старше последнего снимка соответственно отбрасываются.
    \item В процессе работы лидер хранит несколько последних модификаций в памяти и рассылает их репликам в соответствии с общей схемой алгоритма.
        Но в случае значительного отставания реплике пересылается последний снимок и лог операций начиная с него для последующего восстановления.
\end{itemize}

Также были реализованы следующие оптимизации не меняющие принципиальной схемы работы:
\begin{itemize}
\item heartbeat осуществляется без задержки при клиентском запросе на запись
\item сброс write-ahead log на диск осуществляется фоновым процессом, но при обработке клиентских запросов лидером и обработке heartbeat репликами происходит без задержки.
\end{itemize}

\chapter{Алгоритм репликации состояния}
\section{Инструменты}

Все эксперименты проводились в операционной системе Linux.  Для прикладного использования на данной платформе NVDIMM
после предварительной разметки и настройки представляет собой блочное устройство. Для использования в качестве памяти предполагается создание файла с помощью fallocate
и его отображение в виртуальную память с помощью mmap. Файловая система при этом должна предоставлять режим DAX, отключающий page-cache. Мы в частности использовали Ext4.
После вышеописанных шагов модификации выделенного пространства на NVDIMM не требуют системных вызовов и не задействуют ядро операционной системы.

Записи в NVDIMM кешируются процессором. Для отказоустойчивой записи необходимо вытеснить соответствующие участки памяти из кэша одной из инструкций clflush/clflushopt/clwb
и подождать барьер записи -- например выполнить инструкцию sfence. При этом механизмов предотвращающих преждевременное вытеснение памяти из процессорного кэша нет.
NVDIMM при этом гарантирует атомарность модификации участков памяти, соответствующих кеш-линиям.

В своих экспериментах мы используем библиотеку PMDK, предоставляющую коллекцию базовых примитивов для работы с персистентной памятью.
К примеру такие как указатели на персистентную память, механизм выделения памяти и транзакции в персистентной памяти. О них мы и поговорим подробнее.

В силу механизма отображения персистентной памяти в виртуальную, персистентная память может быть доступна прикладным программам с разным смещением в разных запусках.
А значит, при необходимости использования ссылок на персистентную память необходимо оперировать не указателями, а смещениями внутри персистентного региона памяти.

\label{allocations}
Задача управления персистентной памятью также имеет свою специфику: выделение памяти и запись ссылки на неё в уже выделенную область памяти должны происходить атомарно, так как
при пропаже питания между этими двумя действиями может образоваться утечка памяти. Утечки персистентной памяти наносят гораздо больший вред чем утечки оперативной,
так как не исчезают после перезапуска программы. Механизм выделения памяти в PMDK использует транзакции.

Транзакция в персистентной памяти это атомарное изменение нескольких участков памяти. Отдельно подчеркнём что мы говорим не про атомарность изменения для других процессорных ядер, а
про атомарность изменения в случае сбоя питания.
Обобщённая схема механизма транзакций в PMDK такова:
\begin{enumerate}
    \item модифицируемые регионы копируются в undo log -- примитив реализованный в библиотеке libpmemlog.
    \item модифицируются исходные участки памяти
    \item удаляется запись из undo log
\end{enumerate}
На каждом из шагов все записи отказоустойчиво фиксируются в персистентной памяти (то есть вытесняются из процессорного кеша и ожидается барьер записи).

\section{Структуры данных}
\label{datastructures}
Для уменьшения латентности записи мы отказываемся от write-ahead log и модифицируем структуры данных сразу при обработке пользовательских запросов лидером
и обработке heartbeat репликами. Все структуры данных в наших экспериментах в том или ином виде реализуют сортированный ассоциативный массив.
Мы пользуемся MVCC \cite{bernstein1983multiversion} храня несколько версий значений -- ключами в этом массиве выступают пары (пользовательский ключ, raft timestamp).
Также для каждой неподтверждённой кворумом транзакции мы храним запись для отката со списком указателей на её ключи. Ключ для каждой такой записи имеет вид
(зарезервированная строка, timestamp транзакции).
Для операции $get(x)$ мы выполняем поиск наибольшего ключа не превосходящего (x, applied\_ts) где applied\_ts -- номер последней применённой транзакции в терминах raft.
Для операции $compare\_and\_set(x, value_1, value_2)$ соответственно выполняем поиск максимального ключа вида $(x, y).$

При работе алгоритма лидер хранит в DRAM некоторое небольшое количество записей соответствующих последним обработанным транзакциям для пересылки репликам.
Следуя общей схеме алгоритма RAFT лидер с некоторой периодичностью пытается переслать эти записи репликам. В случае значительного отставания какой-либо реплики, она не может
принять записи из вышеописанного буфера, так как получала промежуточных записей от лидера. Для эффективного решения задачи восстановления таких реплик все реализованные нами
структуры данных имеют механизм снимков состояния для последующей пересылки отставшей реплике состояния лидера целиком.

В качестве структуры данных реализующей ассоциативный массив мы пробовали персистентные B+ деревья. В наших тестовых сценариях размер базы составлял сотни тысяч ключей и
оптимально с точки зрения производительности себя показали деревья с фактором ветвления 4-7.

Также были реализованы персистентные односвязные списки. Для ускорения операций чтения в DRAM поддерживается сбалансированное дерево поиска со ссылками на вершины данного списка.
Модификации (как записи так и удаления) в данной структуре данных реализованы добавлением в конец списка и соответствующими модификациями дерева в DRAM.
Подчеркнём сходство данной схемы с классическим write-ahead log. Ключевое отличие данной структуры данных в возможности очистки такого списка от дубликатов ключей фоновым
процессом без блокировки процесса чтений и модификаций и значительной нагрузки на устройство хранения. Снимком данной структуры данных является односвязный список.
В нашей реализации взятие снимка блокирует процесс очистки структуры данных от дубликатов.

\section{Управление памятью}
\label{gc}
Как упоминалось в \ref{allocations}, управление персистентной памятью представляет из себя нетривиальную задачу.
В отличие от PMDK мы реализовали систему с автоматической сборкой мусора. Так как все реализованные структуры являются персистентными, это позволяет не вытеснять данные из кеша
при каждой модификации структуры данных, а делать это для групп модификаций. Здесь подчеркнём что мы можем позволить себе гораздо меньшие размеры
таких групп чем в случае использования write-ahead logging. Более того, персистентность структур данных даёт возможность реализации неблокирующих алгоритмов сборки.
Далее опишем общую схему реализации.

При инициализации базы мы размечаем всю доступную персистентную память на страницы -- в наших тестах оптимально себя показали размеры порядка 1K.
В начале каждой страницы находится контрольный блок -- указатель на свободную память в этой странице и указатель на следующую в односвязном списке страниц.
Кроме страниц в персистентной памяти находится общий контрольный блок с указателями на два списка: занятых и полностью свободных страниц соответственно и
указателем на одну из реализаций ассоциативного массива приведённых в \ref{datastructures}.

При выделении памяти либо сдвигается указатель в странице из головы списка ``занятых'' страниц, либо перемещается страница из списка свободных в список занятых и также сдвигается указатель. Операции со списками выполняются с помощью примитива транзакций из PMDK, все остальные операции выполняются без вытеснения из процессорного кеша. Для подтверждения записи группы
модификаций (и соответственно аллокаций) выполняется вытеснение из кеша указателей всех задействованных страниц и участков памяти, соответствующих модификациям
и ожидается барьер записи в NVDIMM. После этого аналогично обновляется указатель на структуру данных в контрольном блоке.

Для сборки мусора в контрольном блоке хранится $stale\_head$ -- указатель на голову списка занятых страниц на момент последнего сохранения указателя на персистентную структуру данных.
Сборщик мусора пересекает адреса которые доступны обходом структуры из контрольного блока со списком страниц начиная с $stale\_head$ (не включая первую страницу).
Все страницы не вошедшие в пересечение перемещаются в список свободных. Модификации и соответственно чтения для сборки мусора из контрольного блока защищены
мьютексом в оперативной памяти. В реализации персистентного списка из \ref{datastructures} во время сборки мусора также производится дедупликация записей в списке.

В \ref{pmemkvtests} находятся результаты сравнения наших алгоритмов основанных на данной схеме с алгоритмами из pmemkv \cite{pmemkv}.

\section{Алгоритм}
Схема работы нашего алгоритма повторяет описанную в \cite{raftpaper}. Удалению записей соответствуют откаты из нашего ассоциативного массива с помощью дополнительных записей описанных в \ref{datastructures}. Состоянию конечного автомата отвечает срез множества ключей по применённому timestamp хранимому в DRAM. Модификации структуры производятся в момент обработки
клиентского запроса лидером и получения новых записей репликами. Фиксация на NVDIMM, описанная в \ref{gc} осуществляется каждые $k$ модификаций и с некоторой частотой.
В наших тестах оптимальными оказались $k=10$ и частота 1us. В момент увеличения применённого timestamp в терминах RAFT лидером отправляется подтверждение клиенту и после удаляются
старые версии значений ключей из ассоциативного массива а также записи для отката соответствующих изменений. Фиксация удалений перед подтверждением не ожидается.

При восстановлении отставшей реплики пересылается полный снимок структуры данных. Номера применённого timestamp и последнего содержатся в снимке.

\chapter{Результаты экспериментов}
\section{Тесты алгоритмов консенсуса}
Эксперименты производились на Intel Optane DC Persistent Memory.
Размер базы в экспериментах -- 50000 ключей. Ключи и значения занимают порядка 10 байт.

Далее  WAL -- классическая реализация raft, описанная в \ref{baseline}. list -- реализация персистентного списка, описанная в \ref{datastructures}
и btree -- с персистентными B+ деревьями (приведены тесты для 4-7 детей).

Для выяснения минимальной возможной латентности мы проводили эксперименты в которых системе задавались запросы последовательно, каждый после того как получен ответ на предыдущий запрос
На одной машине (без сетевых задержек) медиана времени ответа:
\begin{center}
\begin{tabular} {|c c c|}
\hline
    WAL & List & btrees \\
    \hline
150us & 100us & 120us \\
\hline
\end{tabular}
\end{center}

На разных машинах (с сетевыми задержками):
\begin{center}
\begin{tabular} {|c c c|}
\hline
    WAL & List & btrees \\
    \hline
257us & 230us & 250us \\
\hline
\end{tabular}
\end{center}

Также аналогичные тесты проводили с 20 потоками задающими запрос.

На одной машине

\begin{center}
\begin{tabular} {|l |c c c|}
\hline
квантиль & WAL & List & btrees \\
\hline
0.5 & 560us & 412us & 480us \\
0.9 & 800us & 650us & 780us \\
\hline
\end{tabular}
\end{center}

На разных машинах:

\begin{center}
\begin{tabular} {|l |c c c|}
\hline
квантиль & WAL & List & btrees \\
\hline
0.5 & 720us & 610us & 700us \\
0.9 & 1120us & 800us & 1020us \\
\hline
\end{tabular}
\end{center}

Время ответа на операции чтения ожидаемо оказалось одинаковым в пределах погрешности.

Также нами производились тесты recovery в конфигурации с тремя участниками.
Один из них участников выключался, после производилось 15 тысяч записей по единицам килобайт.
Мы измеряли время восстановления отставшей реплики (пересылка снепшота и т.п.) и время старта приложения при соответствующем размере базы.

\begin{center}
\begin{tabular} {|l |c c c|}
\hline
 & WAL & List & btrees \\
\hline
восстановление реплики & 600ms & 100ms & 350ms \\
старт приложения & 200ms & 40ms & 20ms \\
\hline
\end{tabular}
\end{center}


\section{Тесты структур данных}
\label{pmemkvtests}

Мы сравнивали производительность структур данных из \ref{datastructures} со следующими структурами, реализованными в
библиотеке pmemkv \cite{pmemkv}: cmap, stree, tree3.

cmap представляет из себя реализацию хеш-таблицы оптимизированной для параллельного доступа \cite{malakhov2015perbucket}, расположенною полностью в персистентной памяти.
stree -- сортированное B+ дерево, все вершины которого кроме листовых расположены в оперативной памяти.
tree3 -- несортированное B+ дерево оптимизированное для чтений.

Тесты производились на состоянии из 20000 ключей.

\begin{center}
\begin{tabular} {|l| c c c c c|}
\hline
среднее время/ns & cmap & stree & tree3 & list & btrees \\
\hline
вставка & 9397 & 4192 & 4958 & 884 & 3097 \\
чтение & 1767 & 389 & 319 & 340 & 500 \\
удаление & 6201 & 1740 & 2857 & 612 & 3839 \\
\hline
\end{tabular}
\end{center}

Подтверждение модификаций производилось каждые десять модификаций. Заметим что в отличие от наших структур данных, pmemkv ожидает подтверждение для каждой операции.

\chapter{Заключение}
В наших тестах значительную долю времени ответа распределённой системы составляли сетевые задержки и принципиальное улучшение времени отклика возможно было бы скорее с помощью
пересмотра механизма пересылки сообщений. TCP, который мы использовали показал себя не с лучшей стороны, внося как сравнительно большие задержки -- порядка сотен микросекунд, так
и обеспечивая высокую дисперсию времени передачи сообщений -- тоже сотни микросекунд. Более того, tcp потребляет сравнительно много процессорного времени, требуя на одно только
формирование пакетов также единицы микросекунд процессорного времени. Однако, ускорение recovery с помощью NVDIMM вполне достижимо. Более того, 
пересмотр алгоритмов позволяет понизить write amplification, избавляясь от отдельной записи снимков.


\bibliography{report}
\bibliographystyle{plain}


\end{document}
