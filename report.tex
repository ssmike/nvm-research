\documentclass[pdftex,ptm,12pt,a4paper]{report}
\renewcommand{\baselinestretch}{1.5}
\setcounter{secnumdepth}{5}

% PDF search & cut'n'paste
\usepackage{cmap}
\usepackage[table,xcdraw]{xcolor}
\renewcommand{\baselinestretch}{1.5}
\usepackage{setspace}
\usepackage{indentfirst}

% Cyrillic support
\usepackage{mathtext}
\usepackage{amsmath}
\usepackage[T1,T2A]{fontenc}
\DeclareSymbolFont{T2Aletters}{T2A}{cmr}{m}{it}
\usepackage[utf8]{inputenc}
\usepackage{multicol}

\usepackage[english, russian]{babel}

\usepackage[bottom=30mm,top=20mm,right=20mm,left=30mm,headsep=0cm,nofoot]{geometry}

\usepackage{calc}
\setlength{\footskip}{\paperheight
  -(1in+\voffset+\topmargin+\headheight+\headsep+\textheight)
  -0.75in}

\usepackage{array}
\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}

\makeatletter
\renewcommand*{\ps@plain}{%
  \let\@mkboth\@gobbletwo
  \let\@oddhead\@empty
  \def\@oddfoot{%
    \reset@font
    \hfil
    \thepage
    % \hfil % removed for aligning to the right
  }%
  \let\@evenhead\@empty
  \let\@evenfoot\@oddfoot
}
\makeatother
\pagestyle{plain}

\usepackage[pdftex]{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage[russian,english]{babel}
    \addto{\captionsenglish}{\renewcommand{\bibname}{Литература}}
    \addto\captionsenglish{\renewcommand{\figurename}{Рис.}}
    \addto\captionsenglish{\renewcommand{\contentsname}{Содержание}}
    \addto\captionsenglish{\renewcommand{\proofname}{Доказательство}}
\usepackage{hyperref}
\usepackage{cleveref}
\usepackage{url}
\usepackage{abstract}
\usepackage{float}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{amsmath}
\renewcommand*{\proofname}{Доказательство}
\usepackage{indentfirst}
\usepackage{color}
\usepackage{natbib}
\usepackage{bbm, dsfont}


% Detect whether PDFLaTeX is in use
\usepackage{ifpdf}

% Fix links to floats
\usepackage[all]{hypcap}

\makeatletter
\renewcommand{\@chapapp}{Часть}
\makeatother

% Theorem Styles
\newtheorem{theorem}{Теорема}[chapter]
\newtheorem{lemma}[theorem]{Лемма}
\newtheorem{claim}[theorem]{Теорема}
% Definition Styles
\theoremstyle{definition}
\newtheorem{definition}{Определение}[chapter]
\newtheorem{example}{Пример}[chapter]
% Rule for Title Page
\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}

\begin{document}

\begin{titlepage}
\newpage

\begin{center}
МИНИСТЕРСТВО ОБРАЗОВАНИЯ И НАУКИ РОССИЙСКОЙ ФЕДЕРАЦИИ \\
\vspace{0.5cm}
ГОСУДАРСТВЕННОЕ ОБРАЗОВАТЕЛЬНОЕ УЧРЕЖДЕНИЕ \\*
ВЫСШЕГО ПРОФЕССИОНАЛЬНОГО ОБРАЗОВАНИЯ\\*
"МОСКОВСКИЙ ФИЗИКО-ТЕХНИЧЕСКИЙ ИНСТИТУТ \\*
(ГОСУДАРСТВЕННЫЙ УНИВЕРСИТЕТ)" \\*
\vspace{0.5cm}
ФАКУЛЬТЕТ ИННОВАЦИЙ И ВЫСОКИХ ТЕХНОЛОГИЙ \\*
КАФЕДРА АНАЛИЗА ДАННЫХ \\*
\hrulefill
\end{center}


\vspace{3em}

\begin{center}
\Large Выпускная квалификационная работа по направлению 01.03.02 <<Прикладная математика и информатика>> \linebreak НА ТЕМУ:
\end{center}

\vspace{2.5em}

\begin{center}
\textsc{\large{\textbf{Алгоритмы консенсуса на энергонезависимой памяти произвольного доступа}}}
\end{center}

\vspace{6.5em}

\begin{minipage}{.45\linewidth}
\begin{flushleft}
Студент \\ Научный руководитель к.ф-м.н \\ Зам. зав. кафедрой д.ф-м.н, проф.
\end{flushleft}
\end{minipage}
\hfill
\begin{minipage}{.45\linewidth}
\begin{flushright}
    Сурин М.С.\\ Бабенко А.В.\\Бунина Е.И.
\end{flushright}
\end{minipage}

\vspace{\fill}

\begin{center}
МОСКВА, 2020
\end{center}

\end{titlepage}

\tableofcontents
\sloppy

\setstretch{1.5}
\parindent=1.25cm

\chapter{Введение}
\section{NVDIMM}
Разработчии СУБД всегда был вынуждены принимать во внимание принципиальную разницу между энергозависимыми устройствами хранения информации и энергонезависимыми.
Разница между типами устройств проявляется как в латентности доступа, так и в особенностях предоставляемого интерфейса.
Энергонезависимые устройства как правило предоставляют блочный интерфейс, то есть
позволяют оперировать только блоками из тысяч байт, в то время как современные DIMM дают возможность оперировать словами -- единицами байт.
Но не так давно (2014-15гг) на рынке появились технологии под общим названием “NVDIMM”, занимающие промежуточное положение: они сохраняют данные при потере питания,
предоставляя скорость доступа, сравнимую с DRAM, а также дают возможность адресации отдельных байт.

Большая часть продуктов на 2014г (например продукты Miсron Technology) представляют из себя модуль DRAM служащий кешeм для модуля энергонезависимой памяти (как правило NAND FLASH)
с автономным источником питания. Но тогда же были анонсированы, а позже и появились на рынке устройства без подобного разделения -- к примеру Intel Optane NVDIMM \cite{peng2019system}.

Типичные времена выполнения операция для промышленных устройств разных типов:
\begin{center}
\begin{tabular} {|l| c c c|}
\hline
    & SSD & NVDIMM & DIMM \\
    \hline
чтение & 200us & 120ns & 80ns \\
запись & 2ms & 750ns & 80ns \\
\hline
\end{tabular}
\end{center}

Такая небольшая разница между оперативной памятью и энергонезависимой подталкивает исследователей к пересмотру традиционных подходов к дизайну и архитектуре СУБД и перспективе даёт возможность добиться большей производительности.

\section{Обзор литературы}
Ещё до появления на рынке на рынке устройств, исследователи пытались оптимизировать алгоритмы \cite{iwabuchi2014nvm} и структуры данных \cite{chen2015persistent} для персистентной памяти .
Многие из них пользуются либо программными платформами для эмуляции задержек NVDIMM \cite{sengupta2015framework} либо занимаются построением аппаратных моделей \cite{dong2012nvsim}.
Использование энергонезависимой памяти ставит перед исследователями новые задачи, такие как менеджмент персистентной памяти \cite{schwalb2015nvm}. Также своя специфика есть у задачи обеспечения
целостности данных при потере питания, так как большинство современных процессоров кешируют доступы к памяти и практически не предоставляют примитивов для управления своим кешем.
Как следствие для работы с персистентной памятью появились отдельные библиотеки, такие как уже ставшая стандартом для исследователей библиотека PMDK \cite{pmdk}
и ряд процессорных инструкций \cite{kolli2016delegated}.  Мы в своей работе, в частности, используем PMDK и сравниваем наши алгоритмы с построенной на её базе библиотекой pmemkv, 
\cite{pmemkv} как и большинство исследователей персистентных структур данных.
Задачей оптимизации архитектуры СУБД на персистентной памяти занимается крайне малое количество исследователей; в основном можно выделить исследователей из Carnegie Mellon School of Computer Science
\cite{pavlo17}, \cite{arulraj2015let}, \cite{debrabant2014prolegomenon}, \cite{arulraj2017build}, \cite{writebehind}.  В \cite{writebehind} приведeна альтернатива write-ahead logging
для NVM, позволяющая повысить производительность СУБД и рассмотрена задача репликации с учётом NVM, но задача координации -- выбора master-реплики перекладывалась на внешнее отказоустойчивое хранилище.
Задачей исследования алгоритмов консенсуса для NVM по нашим сведениям никто не занимался.

\chapter{Цель работы}
\section{Постановка задачи}
Мы рассматриваем алгоритмы консенсуса с точки зрения задачи репликации состояния конечного автомата. Состояние будет представлять из себя ассоциативный массив, ключами и значениями
выступают произвольные последовательности байт. Зафиксируем интерфейс, достаточный для большинства прикладных задач:
\begin{itemize}
    \item get($key$) -- операция чтения значения по ключу
    \item set($key$, $value$) -- операция записи значения по ключу
    \item compare\_and\_set($key$, $value_1$, $value_2$) -- операция обновления значения по ключу $key$ на $value_2$ в случае если оно до операции равно $value_1$
\end{itemize}

От алгоритмов будем требовать линеаризуемости \cite{linearizability} истории операций в условиях наличия нарушений связности сети.

\section{Метрики}
При экспериментах мы изучаем следующие характеристики:
\begin{itemize}
    \item латентность записи (то есть операций set и compare\_and\_set)
    \item скорость восстановления отставших реплик
\end{itemize}

В большинстве прикладных реализаций алгоритмов репликации, операции чтения обслуживается лидером напрямую из своей оперативной памяти, не задействуя другие реплики.
А значит, от алгоритмов оптимизированных для NVM нет поводов ожидать улучшений латентности чтения. Поэтому в нашей работе  мы данную метрику не исследуем.

\section{Бейзлайн}
В качестве эталонной реализации мы используем реализацию алгоритма RAFT \cite{raftpaper} использующую NVDIMM как блочное устройство
для хранения write-ahead log и снимков состояния.

\label{baseline}
Напомним некоторые детали общей схемы работы данного алгоритма:
\begin{itemize}
    \item Алгоритм реплицирует состояние некоторого конечного автомата.
    \item Каждое изменение состояния (мутация) получает уникальный номер от лидера -- далее timestamp.
    \item От лидера каждое изменение пересылается репликам.
    \item Реплики отказоустойчиво сохраняют присланные изменения (типично в WAL)
        и отправляют лидеру timestamp последнего сохранённого изменения вместе с timestamp следующего, ещё не
        полученного от лидера изменения. Изменения на репликах, отличающиеся от изменений полученных от лидера, при нахождении удаляются вместе
        с последующими сохранёнными соответствующей репликой (подобные ситуации могут возникнуть в случае сбоев).
    \item Лидер применяет изменение к своему состоянию после подтверждения того что оно сохранено более чем половиной (кворумом) участников.
    \item Лидер и реплики периодически обмениваются сообщениями -- далее heartbeat; в случае неполучения такого сообщения в течение некоторого временного периода реплики инициируют перевыборы лидера.
    \item При выборах каждый участник пересылает предложение о голосовании за себя остальным, включая в него timestamp последнего сохранённого сообщения. В случае получения предложения с меньшим
    timestamp чем у получателя предложение отвергается. Новым лидером становится реплика, за которую проголосовало более половины участников. До того как приступить к обслуживанию
    запросов новый лидер рассылает и после применяет все свои изменения.
\end{itemize}

Кратко напомним общую схему работы write-ahead log:
\begin{itemize}
    \item Модификации накапливаются в памяти и периодическим процессом записываются на диск.
        Буфферизация записи делается для оптимизации паттерна нагрузки на диск и следовательно увеличения пропускной способности.
        Записи на диск подтверждаются одним из примитивов предоставляемых платформой (fsync/fdatasync в случае Linux).
    \item В случае пропажи питания и последующем перезапуске процесса, состояние процесса восстанавливается из записанных на диск мутаций. В качестве оптимизации
        с некоторой периодичностью на диск записываются снимки состояния автомата в памяти и при восстановлении используются только мутации старше последнего снимка.
    \item В процессе работы лидер хранит несколько последних модификаций в памяти и рассылает их репликам в соответствии с общей схемой алгоритма.
        В случае значительного отставания реплике пересылается последний снимок и лог операций начиная с него для последующего восстановления.
\end{itemize}

Также были реализованы следующие оптимизации не меняющие принципиальной схемы работы:
\begin{itemize}
\item heartbeat осуществляется без задержки при клиентском запросе на запись
\item сброс write-ahead log на диск осуществляется фоновым процессом, но при обработке клиентских запросов лидером и обработке heartbeat репликами происходит без задержки.
\end{itemize}

\chapter{Алгоритм репликации состояния}
\section{Инструменты}

Все эксперименты проводились в операционной системе Linux. После предварительной разметки и настройки NVDIMM в системе доступна как блочное устройство.
Для использования его в качестве памяти требуется создание файла с помощью fallocate
и его отображение в виртуальную память с помощью mmap. Файловая система при этом должна предоставлять режим DAX, отключающий page-cache. Мы использовали файловую систему Ext4, имеющую данный режим.
После выполнения вышеописанных шагов модификации выделенного пространства на NVDIMM не требуют системных вызовов и не задействуют ядро операционной системы.

Как и при работе с DRAM, записи в NVDIMM кешируются процессором.
Для отказоустойчивой записи необходимо вытеснить соответствующие участки памяти из кэша одной из инструкций clflush/clflushopt/clwb
и подождать барьер записи -- например выполнить инструкцию sfence.
NVDIMM при этом гарантирует атомарность модификации участков памяти, соответствующих кеш-линиям.
Механизмов предотвращающих преждевременное вытеснение памяти из процессорного кэша система не предоставляет.

В своих экспериментах мы используем библиотеку PMDK, предоставляющую коллекцию базовых примитивов для работы с персистентной памятью.
К примеру, например указатели на персистентную память, аллокации и транзакций в персистентной памяти. Рассмотрим данные примитивы подробнее.

В силу механизма отображения персистентной памяти в виртуальную, персистентная память может быть доступна прикладным программам с разным смещением в разных запусках.
А значит, при необходимости использования ссылок на персистентную память необходимо оперировать не указателями, а смещениями внутри персистентного региона памяти.

\label{allocations}
Задача управления персистентной памятью также имеет свою специфику: выделение памяти и запись ссылки на неё в уже выделенную область памяти должны происходить атомарно, так как
при пропаже питания между этими двумя действиями может образоваться утечка памяти. Утечки персистентной памяти имеют гораздо более серьёзные последствия, чем утечки оперативной,
так как не исчезают после перезапуска программы. Механизм выделения памяти в PMDK использует транзакции.

Транзакция в персистентной памяти это атомарное изменение нескольких участков памяти. Отдельно подчеркнём, что мы говорим не про атомарность изменения с точки зрения видимости
для других процессорных ядер, а
про атомарность изменения в случае сбоя питания.
Обобщённая схема механизма транзакций в PMDK такова:
\begin{enumerate}
    \item модифицируемые участки памяти копируются в undo log -- примитив реализованный в библиотеке libpmemlog.
    \item модифицируются исходные участки памяти
    \item удаляется запись из undo log
\end{enumerate}
На каждом из шагов все записи отказоустойчиво фиксируются в персистентной памяти (то есть вытесняются из процессорного кеша и ожидается барьер записи).

\section{Структуры данных}
\label{datastructures}
Для уменьшения латентности записи мы отказываемся от записи отдельного write-ahead log и модифицируем структуры данных сразу во время обработки лидером пользовательских запросов
и обработке heartbeat-запросов репликами. 
Мы пользуемся подходом MVCC \cite{bernstein1983multiversion} -- а именно, храним несколько версий значений для каждого пользовательского ключа.

Все структуры данных в наших экспериментах в том или ином виде реализуют сортированный ассоциативный массив -- ключами в этом массиве выступают пары (пользовательский ключ, raft timestamp).
Также для каждой неподтверждённой кворумом мутации мы храним запись для отката со списком указателей на её ключи. Ключ для такой записи имеет вид
(зарезервированная строка, timestamp транзакции).
Для операции $get(x)$ мы выполняем поиск наибольшего ключа не превосходящего (x, applied\_ts) где applied\_ts -- номер последней применённой транзакции в терминах raft.
Для операции $compare\_and\_set(x, value_1, value_2)$ соответственно выполняем поиск максимального ключа вида $(x, y).$

При работе алгоритма лидер хранит в DRAM некоторое небольшое количество записей соответствующих последним обработанным мутациям для пересылки репликам.
Следуя общей схеме алгоритма RAFT лидер с некоторой периодичностью пытается переслать эти записи репликам. В случае значительного отставания какой-либо реплики, она не может
принять записи из вышеописанного буфера, так как не получала промежуточных записей от лидера. Для эффективного решения задачи восстановления таких реплик все реализованные нами
структуры данных имеют механизм снимков состояния для последующей пересылки отставшей реплике состояния лидера целиком.

В работе было реализовано две структуры данных, реализующих ассоциативный массив. В первой версии мы пробовали персистентные B+ деревья.
В наших тестовых сценариях размер базы составлял сотни тысяч ключей и наилучшим образом с точки зрения производительности себя показали деревья с фактором ветвления 4-7.

Во второй версии была реализована комбинированная структура данных, частично размещающаяся в DRAM и частично в NVDIMM. В NVDIMM размещается персистентный односвязный спискок.
Для ускорения операций чтения в DRAM поддерживается сбалансированное дерево поиска со ссылками на вершины данного списка.
Модификации (как записи так и удаления) в данной структуре данных реализованы добавлением в конец списка и соответствующими модификациями дерева в DRAM.
Подчеркнём сходство данной схемы с классическим write-ahead log. Ключевое отличие данной структуры данных в возможности очистки такого списка от дубликатов ключей фоновым
процессом, без блокировки процесса чтений и модификаций и значительной нагрузки на устройство хранения,  а также в том, что пользовательские значения не дублируются
на NVDIMM и в DRAM.
Снимком данной структуры данных является односвязный список.
В нашей реализации взятие снимка блокирует процесс очистки структуры данных от дубликатов.

\section{Управление памятью}
\label{gc}
Как упоминалось в \ref{allocations}, управление персистентной памятью представляет из себя нетривиальную задачу.
В отличие от PMDK мы не используем транзакционное выделение памяти на каждую аллокацию, вместо этого мы реализовали систему с автоматической сборкой мусора.
Персистентность структур данных даёт возможность реализации неблокирующих алгоритмов сборки мусора и в целом упрощает алгоритм.
Далее опишем общую схему реализации. 

При инициализации мы размечаем всю доступную персистентную память на страницы -- в наших тестах оптимально себя показали размеры порядка 1K.
В начале каждой страницы находится контрольный блок -- указатель на свободную память в этой странице и указатель на следующую страницу в односвязном списке страниц.
Кроме страниц в персистентной памяти находится общий контрольный блок с указателями на два списка: занятых и полностью свободных страниц соответственно и
указателем на одну из реализаций ассоциативного массива приведённых в \ref{datastructures}.

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{pages.png}
\caption{Схема управления памятью.}
\label{mempic}
\end{figure}

При выделении памяти сдвигается указатель в странице из головы списка ``занятых'' страниц; а также при необходимости страница из списка свободных перемещается в список занятых.
Операции со списками выполняются с помощью примитива транзакций из PMDK, но все остальные операции выполняются без многократных вытеснений из процессорного кеша.
Для подтверждения записи группы модификаций (и соответственно аллокаций) выполняется вытеснение из кеша указателей всех задействованных страниц и участков памяти,
соответствующих модификациям и ожидается барьер записи в NVDIMM. Персистентность структуры данных, реализующей ассоциативный массив, даёт возможность вытеснять только память выделенную
с момента последнего подтверждения (``старые'' участки памяти не изменяются).
После этого аналогичным образом обновляется указатель на структуру данных в контрольном блоке.
На рис. \ref{mempic} изображена схема структуры данных на nvdimm: красным изображён контрольный блок, синим -- занятые участки страниц, зелёным -- занятые с момента последнего
подтверждения.
 
Для сборки мусора в контрольном блоке хранится $stale\_head$ -- указатель на голову списка занятых страниц на момент последнего сохранения указателя на персистентную структуру данных.
Сборщик мусора строит пересечение списка страниц (начиная с $stale\_head$ и исключая первую страницу) с адресами которые доступны обходом структуры из контрольного блока со списком страниц.
Все страницы не вошедшие в пересечение перемещаются в список свободных. Модификации и соответственно чтения контрольного блока защищены мьютексом в оперативной памяти.
В реализации персистентного списка из \ref{datastructures} во время сборки мусора также производится дедупликация записей в списке.

Вышеописанная схема позволяет избежать вытеснений из процессорного кеша и ожидания барьеров записи при каждой модификации структуры данных, а делать это для групп модификаций.
Подчеркнём что мы можем позволить себе меньшие размеры таких групп чем в случае использования write-ahead logging в силу разницы между механизмами подтверждения записи --
в случае write-ahead logging на каждую запись производится один и более системных вызовов, а также выполняются операции связанные с механизмом файловой системы.
Выбранная схема управления памятью также помогает лучше утилизировать устройство, так как NVDIMM обрабатывает модификации блоками по 256 байт \cite{izraelevitz2019basic}.

В разделе \ref{pmemkvtests} находятся результаты сравнения производительности ассоциативных массивов основанных на данной схеме управления памятью, с алгоритмами из pmemkv \cite{pmemkv}.

\section{Алгоритм}
Схема работы нашего алгоритма повторяет описанную в \cite{raftpaper}. Удалению записей соответствуют откаты из ассоциативного массива с помощью дополнительных записей описанных в \ref{datastructures}. Состоянию конечного автомата отвечает срез множества ключей по применённому timestamp хранимому в DRAM. Модификации структуры производятся в момент обработки
клиентского запроса лидером и получения новых записей репликами. Фиксация на NVDIMM, описанная в \ref{gc} осуществляется каждые $k$ модификаций и с некоторой частотой фоновым процессом.
В наших тестах оптимальными оказались $k=10$ и частота 1us. В момент увеличения применённого timestamp в терминах RAFT лидером отправляется подтверждение клиенту и после удаляются
старые версии значений ключей из ассоциативного массива а также записи для отката соответствующих изменений. Фиксация удалений перед подтверждением не ожидается.

При восстановлении отставшей реплики пересылается полный снимок структуры данных. Номера применённого timestamp и последнего содержатся в снимке.

\chapter{Результаты экспериментов}
\section{Тесты алгоритмов консенсуса}
Эксперименты производились на Intel Optane DC Persistent Memory.
Размер базы в экспериментах -- 50000 ключей. Ключи и значения занимают порядка 10 байт.

Далее  WAL -- классическая реализация raft, описанная в \ref{baseline}. list -- реализация персистентного списка, описанная в \ref{datastructures}
и btree -- с персистентными B+ деревьями (приведены тесты для 4-7 детей).

Для выяснения минимальной возможной латентности мы проводили эксперименты в которых системе задавались запросы последовательно, каждый после того как получен ответ на предыдущий запрос
На одной машине (без сетевых задержек) медиана времени ответа:
\begin{center}
\begin{tabular} {|c c c|}
\hline
    WAL & List & btrees \\
    \hline
150us & 100us & 120us \\
\hline
\end{tabular}
\end{center}

На разных машинах (с сетевыми задержками):
\begin{center}
\begin{tabular} {|c c c|}
\hline
    WAL & List & btrees \\
    \hline
257us & 230us & 250us \\
\hline
\end{tabular}
\end{center}

Также аналогичные тесты проводили с 20 потоками задающими запрос.

На одной машине

\begin{center}
\begin{tabular} {|l |c c c|}
\hline
квантиль & WAL & List & btrees \\
\hline
0.5 & 560us & 412us & 480us \\
0.9 & 800us & 650us & 780us \\
\hline
\end{tabular}
\end{center}

На разных машинах:

\begin{center}
\begin{tabular} {|l |c c c|}
\hline
квантиль & WAL & List & btrees \\
\hline
0.5 & 720us & 610us & 700us \\
0.9 & 1120us & 800us & 1020us \\
\hline
\end{tabular}
\end{center}

Время ответа на операции чтения ожидаемо оказалось одинаковым в пределах погрешности.

Также нами производились тесты recovery в конфигурации с тремя участниками.
Один из них участников выключался, после производилось 15 тысяч записей по единицам килобайт.
Мы измеряли время восстановления отставшей реплики (пересылка снепшота и т.п.) и время старта приложения при соответствующем размере базы.

\begin{center}
\begin{tabular} {|l |c c c|}
\hline
 & WAL & List & btrees \\
\hline
восстановление реплики & 600ms & 100ms & 350ms \\
старт приложения & 200ms & 40ms & 20ms \\
\hline
\end{tabular}
\end{center}


\section{Тесты структур данных}
\label{pmemkvtests}

Мы сравнивали производительность структур данных из \ref{datastructures} со следующими структурами, реализованными в
библиотеке pmemkv \cite{pmemkv}: cmap, stree, tree3.

\begin{itemize}
\item cmap является реализацией хеш-таблицы оптимизированной для параллельного доступа \cite{malakhov2015perbucket}, расположенною полностью в персистентной памяти.
\item stree -- сортированное B+ дерево, все вершины которого кроме листовых расположены в оперативной памяти.
\item tree3 -- несортированное B+ дерево оптимизированное для чтений.
\end{itemize}

Тесты производились на состоянии из 20000 ключей размера 10 байт.

\begin{center}
\begin{tabular} {|l| c c c c c|}
\hline
среднее время/ns & cmap & stree & tree3 & list & btrees \\
\hline
вставка & 9397 & 4192 & 4958 & 884 & 3097 \\
чтение & 1767 & 389 & 319 & 340 & 500 \\
удаление & 6201 & 1740 & 2857 & 612 & 3839 \\
\hline
\end{tabular}
\end{center}

Подтверждение модификаций производилось каждые десять модификаций. Заметим что в отличие от наших структур данных, pmemkv ожидает подтверждение для каждой операции.

\chapter{Заключение}
В наших тестах значительную долю времени ответа распределённой системы составляли сетевые задержки и принципиальное улучшение времени отклика возможно было бы при
пересмотре механизма отправки сообщений. TCP, который мы использовали показал себя не с лучшей стороны, внося как сравнительно большие задержки -- порядка сотен микросекунд, так
и обеспечивая высокую дисперсию времени передачи сообщений -- тоже сотни микросекунд. Более того, TCP потребляет сравнительно много процессорного времени, требуя на одно только
формирование пакетов также единицы микросекунд процессорного времени. Однако, ускорение recovery с помощью NVDIMM вполне достижимо.
Оптимизированные для персистентной памяти алгоритмы позволяют понизить write amplification, избавляясь от отдельной записи снимков.


\bibliography{report}
\bibliographystyle{plain}


\end{document}
